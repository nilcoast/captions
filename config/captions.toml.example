[audio]
sample_rate = 16000
channels = 1
# Ring buffer duration in seconds
buffer_seconds = 30
# Capture microphone input (your voice)
capture_mic = true
# Capture system audio output (other people, media, etc.)
capture_sink = true
# Explicit monitor source name for system audio loopback.
# Leave empty to auto-detect (finds first PulseAudio/PipeWire monitor source).
# Example: "alsa_output.pci-0000_00_1f.3.analog-stereo.monitor"
monitor_device = ""

[whisper]
# Path to whisper.cpp GGML model file
model_path = "~/.local/share/captions/ggml-base.en.bin"
# Chunk duration in milliseconds (how often to run inference)
chunk_ms = 3000
# Overlap in milliseconds (for context continuity)
overlap_ms = 500
# "greedy" or "beam"
strategy = "greedy"
# Number of threads for whisper inference
threads = 4
# Language code (e.g. "en", "auto")
language = "en"

[overlay]
# Font description (Pango format)
font = "Sans Bold 18"
# Text color (CSS format)
text_color = "rgba(255, 255, 255, 0.95)"
# Background color of the caption box
bg_color = "rgba(0, 0, 0, 0.70)"
# Maximum number of visible lines
max_lines = 3
# Margin from screen bottom in pixels
margin_bottom = 60
# Margin from screen sides in pixels
margin_side = 80
# Fade timeout in seconds (0 = no fade)
fade_timeout = 5
# Border radius in pixels
border_radius = 16
# Padding in pixels
padding = 16

[recording]
# Base directory for saved sessions
output_dir = "~/captions"
# Save audio as WAV
save_audio = true
# Save transcript text
save_transcript = true

[summary]
# Enable AI summary generation after session ends
enabled = true
# Path to GGUF model file for summarization
model_path = "~/.local/share/captions/phi-3.1-mini-128k-instruct-q4_k_m.gguf"
# Prompt prepended to transcript
prompt = "Summarize the following transcript concisely, highlighting key points and action items:"
# Number of GPU layers to offload (-1 = all, 0 = CPU only)
gpu_layers = -1
# Max tokens for summary output
max_tokens = 256

[daemon]
socket_path = "/tmp/captions.sock"
